{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9701082-5d2d-47f4-992a-9e881ae82273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "057d2125-07c4-4429-ac35-610f6354ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c52e6a66-2992-4605-adac-9d41488f50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Avoid log(0) by adding a small epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return - np.sum( y_true * np.log(y_pred) +  (1 - y_true) *  np.log( 1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b112f442-c80e-4ead-9f8c-c333c0ddf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n",
    "    \"\"\"\n",
    "\tGradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n",
    "\t\"\"\"\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)  # includes bias weight as w[0]\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        z = np.dot(X, weights)       # (n_samples,)\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        loss = binary_cross_entropy(y, y_pred)\n",
    "        losses.append(round(loss, 4))\n",
    "\n",
    "        error = y_pred - y           # (n_samples,)\n",
    "        dw = np.dot(X.T, error)     # (n_features,)\n",
    "\n",
    "        weights -= learning_rate * dw\n",
    "\n",
    "    return weights.flatten().round(4).tolist(), losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aae99705-7158-4b5d-bbf6-f6919e0f148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0003, 0.4038, 0.3379],\n",
       " [np.float64(2.7726),\n",
       "  np.float64(2.6485),\n",
       "  np.float64(2.533),\n",
       "  np.float64(2.4254),\n",
       "  np.float64(2.325),\n",
       "  np.float64(2.2314),\n",
       "  np.float64(2.1441),\n",
       "  np.float64(2.0625),\n",
       "  np.float64(1.9862),\n",
       "  np.float64(1.9148),\n",
       "  np.float64(1.8479),\n",
       "  np.float64(1.7852),\n",
       "  np.float64(1.7263),\n",
       "  np.float64(1.6708),\n",
       "  np.float64(1.6187),\n",
       "  np.float64(1.5696),\n",
       "  np.float64(1.5232),\n",
       "  np.float64(1.4794),\n",
       "  np.float64(1.438),\n",
       "  np.float64(1.3988)])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), np.array([1, 0, 1, 0]), 0.01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fe165-6d02-403d-adb9-de846afb448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85157b73-4b96-4321-b6e0-b41f63491787",
   "metadata": {},
   "source": [
    "## follow this implementation. Deatails [here](https://chatgpt.com/c/6847646d-305c-8006-88e6-01fc11c771d6) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dad71-bf48-4e4e-a75b-fb12e401f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Avoid log(0) by adding a small epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Gradient descent training for logistic regression using Binary Cross Entropy loss.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "        y (np.ndarray): Binary labels of shape (n_samples,)\n",
    "        learning_rate (float): Step size for parameter updates\n",
    "        iterations (int): Number of gradient descent steps\n",
    "\n",
    "    Returns:\n",
    "        tuple: ([rounded weight values], rounded bias value, [losses rounded to 4 decimal places])\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0.0\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Linear model\n",
    "        linear_output = np.dot(X, weights) + bias\n",
    "        y_pred = sigmoid(linear_output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = binary_cross_entropy(y, y_pred)\n",
    "        losses.append(round(loss, 4))\n",
    "\n",
    "        # Compute gradients\n",
    "        error = y_pred - y\n",
    "        dw = np.dot(X.T, error) / n_samples\n",
    "        db = np.mean(error)\n",
    "\n",
    "        # Update parameters\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return list(np.round(weights, 4)), round(bias, 4), losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
